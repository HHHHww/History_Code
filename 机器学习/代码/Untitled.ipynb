{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_new:\n",
      " ['你 就 收到 了 的 萨芬发 过去 文件 .', '阿斯顿 法国 红酒 看来 去 微软 .']\n",
      "data_new:\n",
      " [[0.5        0.5        0.         0.         0.5        0.5\n",
      "  0.        ]\n",
      " [0.         0.         0.57735027 0.57735027 0.         0.\n",
      "  0.57735027]]\n",
      "特征名字:\n",
      " ['收到', '文件', '看来', '红酒', '萨芬发', '过去', '阿斯顿']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import jieba\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def datasets_demo():\n",
    "    '''\n",
    "    sklearn数据集使用'''\n",
    "    iris=load_iris()\n",
    "    print('鸢尾花数据集:\\n',iris)\n",
    "    print('查看数据集描述:\\n',iris['DESCR'])\n",
    "    print('查看特征值的名字:\\n',iris.feature_names)\n",
    "    print('查看特征值:\\n',iris.data,iris.data.shape)\n",
    "#     数据集划分\n",
    "    x_train,x_testchouqu,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.2,random_state=22)\n",
    "    print('训练集的特征值:\\n',x_train,x_train.shape)\n",
    "    return None\n",
    "def dict_demo():\n",
    "    '''\n",
    "    字典特征抽取'''\n",
    "    \n",
    "    data=[{'city':'北京','temperature':100},{'city':'上海','temperature':60},{'city':'深圳','temperature':30}]\n",
    "    transfer=DictVectorizer(sparse=False)\n",
    "    \n",
    "    data_new=transfer.fit_transform(data)\n",
    "    print(\"data_new:\\n\",data_new)\n",
    "    print('特征名字：\\n',transfer.get_feature_names())\n",
    "    return None\n",
    "def count_demo():\n",
    "    '''\n",
    "    文字特征抽取:CountVecotrizer\n",
    "    '''\n",
    "    data=['life is short,i like like python','life is too long,i dislike python']\n",
    "    transfer =CountVectorizer()\n",
    "    data_new=transfer.fit_transform(data)\n",
    "    print('data_new:\\n',data_new.toarray())\n",
    "    print('特征名字:\\n',transfer.get_feature_names())\n",
    "    return None\n",
    "\n",
    "def count_chinese_demo():\n",
    "    '''\n",
    "    中文文本特征抽取:CountVecotrizer\n",
    "    '''\n",
    "    data=['我 爱 北京 天安门','天安门 上 太阳 升']\n",
    "    transfer =CountVectorizer()\n",
    "    data_new=transfer.fit_transform(data)\n",
    "    print('data_new:\\n',data_new.toarray())\n",
    "    print('特征名字:\\n',transfer.get_feature_names())\n",
    "    return None\n",
    "\n",
    "def count_chinese_demo2():\n",
    "    '''\n",
    "    中文文本特征抽取\n",
    "    '''\n",
    "#     1、将中文文本进行分词\n",
    "    data=['你就收到了的萨芬发过去文件.','阿斯顿法国红酒看来去微软.']\n",
    "    data_new=[]\n",
    "    for sent in data:\n",
    "        data_new.append(cut_word(sent))\n",
    "#     print(data_new)\n",
    "    transfer=CountVectorizer(stop_words=['微软','法国'])\n",
    "    data_final=transfer.fit_transform(data_new)\n",
    "    print('data_new:\\n',data_final.toarray())\n",
    "    print('特征名字:\\n',transfer.get_feature_names())\n",
    "    \n",
    "    return None\n",
    "\n",
    "def cut_word(text):\n",
    "    '''\n",
    "    进行中文分词：'我爱北京天安门'-->'我 爱 北京 天安门'\n",
    "    '''\n",
    "    text=' '.join(list(jieba.cut(text)))\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tfidf_demo():\n",
    "    '''\n",
    "    用于TF-IDF的方法进行文本特征抽取\n",
    "    '''\n",
    "    \n",
    "    #     1、将中文文本进行分词\n",
    "    data=['你就收到了的萨芬发过去文件.','阿斯顿法国红酒看来去微软.']\n",
    "    data_new=[]\n",
    "    for sent in data:\n",
    "        data_new.append(cut_word(sent))\n",
    "#     print(data_new)\n",
    "    transfer=TfidfVectorizer(stop_words=['微软','法国'])\n",
    "    data_final=transfer.fit_transform(data_new)\n",
    "    print('data_new:\\n',data_final.toarray())\n",
    "    print('特征名字:\\n',transfer.get_feature_names())\n",
    "    \n",
    "    return None\n",
    "\n",
    "def minmax_demo():\n",
    "    '''\n",
    "    归一化\n",
    "    '''\n",
    "    data=pd.read_csv(\"F:/机器学习/Python3天快速入门机器学习项目资料/机器学习day1资料/02-代码/dating.txt\")\n",
    "    data=data.iloc[:,:3]\n",
    "    print('data:\\n',data) \n",
    "    transfer=MinMaxScaler()\n",
    "    data_new=transfer.fit_transform(data)\n",
    "    print('data_new:\\n',data_new)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def stand_demo():\n",
    "    '''\n",
    "    标准化\n",
    "    '''\n",
    "    data=pd.read_csv(\"F:/机器学习/Python3天快速入门机器学习项目资料/机器学习day1资料/02-代码/dating.txt\")\n",
    "    data=data.iloc[:,:3]\n",
    "    print('data:\\n',data) \n",
    "    transfer=StandardScaler()\n",
    "    data_new=transfer.fit_transform(data)\n",
    "    print('data_new:\\n',data_new)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def variance_demo():\n",
    "    '''\n",
    "    过滤低方差特征\n",
    "    '''\n",
    "    \n",
    "    data=pd.read_csv('F:/机器学习/Python3天快速入门机器学习项目资料/机器学习day1资料/02-代码/factor_returns.csv')\n",
    "    \n",
    "    data=data.iloc[:,1:-2]\n",
    "    print('data\\n',data) \n",
    "    transfer=VarianceThreshold(threshold=10)\n",
    "    data_new=transfer.fit_transform(data)\n",
    "    print('data_new\\n',data_new,data_new.shape)\n",
    "    \n",
    "#     计算某两个变量之间的相关系数\n",
    "    r1=pearsonr(data['pe_ratio'],data['pb_ratio'])\n",
    "    print('相关系数：\\n',r1)\n",
    "    r2=pearsonr(data['revenue'],data['total_expense'])\n",
    "    print('revenue与total_expense之间的相关性：\\n',r2)\n",
    "\n",
    "    return None\n",
    "\n",
    "def pca_demo():\n",
    "    '''\n",
    "    PCA降维\n",
    "    '''\n",
    "    data=[[2,8,4,5],[6,3,0,8],[5,4,9,1]]\n",
    "    transfer=PCA(n_components=0.95)     #n_components=2\n",
    "    data_new=transfer.fit_transform(data)\n",
    "    print('data_new:\\n',data_new)\n",
    "    return None\n",
    "\n",
    "if __name__=='__main__':\n",
    "#     datasets_demo()\n",
    "#     dict_demo()\n",
    "#     count_demo()\n",
    "#     count_chinese_demo()\n",
    "#     count_chinese_demo2()\n",
    "#     print(cut_word('我爱北京天安门'))\n",
    "    tfidf_demo()\n",
    "#     minmax_demo()\n",
    "#     stand_demo()\n",
    "#     variance_demo()\n",
    "#     pca_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
